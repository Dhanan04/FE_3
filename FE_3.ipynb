{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4abdb832",
   "metadata": {},
   "source": [
    "#Ans1.) \n",
    "\n",
    "Min-Max scaling, also known as normalization, is a technique used to scale numerical features to a fixed range, typically between 0 and 1. It works by subtracting the minimum value of the feature and then dividing by the range of the feature (maximum value minus minimum value). This ensures that all features have the same scale and prevents features with larger magnitudes from dominating the model.\n",
    "\n",
    "Example:\n",
    "Let's say we have a feature representing the age of individuals in a dataset. The ages range from 20 to 60. By applying Min-Max scaling, we can transform these ages to a range between 0 and 1, making them more comparable to other features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decdd919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9628f8fd",
   "metadata": {},
   "source": [
    "#Ans2.) \n",
    "\n",
    "The Unit Vector technique, also known as normalization or L2 normalization, scales each feature vector such that the magnitude of the vector is 1. It divides each feature vector by its Euclidean norm.\n",
    "\n",
    "Example:\n",
    "Consider a dataset where each row represents a vector of features. By applying Unit Vector scaling, each feature vector is divided by its Euclidean norm, ensuring that all feature vectors have a magnitude of 1. This technique is useful when the direction of the vector is more important than its magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b51040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6028876b",
   "metadata": {},
   "source": [
    "#Ans3.) \n",
    "\n",
    "PCA is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving the most important information. It achieves this by finding the principal components, which are linear combinations of the original features that capture the maximum variance in the data.\n",
    "\n",
    "Example:\n",
    "Suppose we have a dataset with many correlated features representing different aspects of a car (e.g., horsepower, weight, fuel efficiency). By applying PCA, we can reduce the dimensionality of the dataset while retaining the most important information. This can help in visualizing the data or improving the performance of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad455c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5acc07d9",
   "metadata": {},
   "source": [
    "#Ans4.) \n",
    "\n",
    "PCA can be used for feature extraction by transforming the original features into a new set of orthogonal features called principal components. These principal components are linear combinations of the original features and capture the maximum variance in the data. By selecting a subset of the principal components, we can effectively extract the most important features from the original dataset.\n",
    "\n",
    "Example:\n",
    "Consider a dataset containing images of handwritten digits. Each image is represented by a large number of pixels. By applying PCA, we can extract the principal components that capture the most variation in the pixel values. These principal components represent the essential features of the images, such as the overall shape and orientation of the digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4c618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01597d4c",
   "metadata": {},
   "source": [
    "#Ans5.) \n",
    "\n",
    "To use Min-Max scaling for preprocessing the data:\n",
    "\n",
    "Identify the numerical features in the dataset, such as price, rating, and delivery time.\n",
    "Calculate the minimum and maximum values for each feature.\n",
    "Apply the Min-Max scaling formula to scale each feature to a range between 0 and 1.\n",
    "Replace the original values of the features with their scaled values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7cc8dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c6d14e1",
   "metadata": {},
   "source": [
    "#Ans6.) \n",
    "\n",
    "To use PCA for reducing the dimensionality of the dataset:\n",
    "\n",
    "Standardize the numerical features in the dataset to ensure they have the same scale.\n",
    "Apply PCA to transform the standardized features into principal components.\n",
    "Determine the number of principal components to retain based on the explained variance ratio or other criteria.\n",
    "Select the subset of principal components with the most significant variance to represent the original features.\n",
    "Use the selected principal components as input features for building the stock price prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e367375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eebf9574",
   "metadata": {},
   "source": [
    "#Ans7.) \n",
    "\n",
    "To perform Min-Max scaling:\n",
    "\n",
    "Calculate the minimum and maximum values of the dataset: min_value = 1, max_value = 20\n",
    "Apply the Min-Max scaling formula to each value: scaled_value = (value - min_value) / (max_value - min_value)\n",
    "Transform the values to the desired range of -1 to 1: scaled_values = [-1, -0.5, 0, 0.5, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce689374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled data: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "min_val = np.min(data)\n",
    "max_val = np.max(data)\n",
    "scaled_data = -1 + 2 * ((data - min_val) / (max_val - min_val))\n",
    "print(\"Scaled data:\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffeaef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ef7c53d",
   "metadata": {},
   "source": [
    "#Ans8.) \n",
    "\n",
    "The number of principal components to retain depends on the desired amount of variance explained and the trade-off between dimensionality reduction and information loss. In practice, one can choose the number of principal components based on the cumulative explained variance ratio. For instance, if the cumulative explained variance ratio reaches a threshold (e.g., 95%), one can stop adding more principal components. However, the choice may also be influenced by domain knowledge and the specific requirements of the predictive task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a749e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da4e64e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
